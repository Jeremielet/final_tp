"""
Module d'entra√Ænement du mod√®le Random Forest avec Optuna et MLflow.

Ce module :
1. Split les donn√©es en train/test
2. Optimise les hyperparam√®tres avec Optuna
3. Entra√Æne le meilleur mod√®le
4. Track tout avec MLflow
5. Sauvegarde le mod√®le
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, confusion_matrix
)
import optuna
import mlflow
import mlflow.sklearn
import pickle
from pathlib import Path


def split_data(X, y, test_size=0.2, random_state=42):
    """
    Split les donn√©es en train et test.

    Args:
        X: Features
        y: Cible
        test_size: Proportion du test set (default: 0.2)
        random_state: Graine al√©atoire (default: 42)

    Returns:
        Tuple (X_train, X_test, y_train, y_test)
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state,
        stratify=y  # Garde la m√™me proportion de fraudes
    )

    print(f"‚úì Donn√©es divis√©es :")
    print(f"  Train : {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)")
    print(f"    - Fraudes : {y_train.sum()} ({y_train.sum()/len(y_train)*100:.2f}%)")
    print(f"  Test  : {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)")
    print(f"    - Fraudes : {y_test.sum()} ({y_test.sum()/len(y_test)*100:.2f}%)")

    return X_train, X_test, y_train, y_test


def calculate_metrics(y_true, y_pred, y_pred_proba):
    """
    Calcule toutes les m√©triques d'√©valuation.

    Args:
        y_true: Vraies valeurs
        y_pred: Pr√©dictions (0 ou 1)
        y_pred_proba: Probabilit√©s de la classe 1

    Returns:
        Dictionnaire avec toutes les m√©triques
    """
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1_score': f1_score(y_true, y_pred),
        'roc_auc': roc_auc_score(y_true, y_pred_proba)
    }

    # Matrice de confusion
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    metrics['true_negatives'] = int(tn)
    metrics['false_positives'] = int(fp)
    metrics['false_negatives'] = int(fn)
    metrics['true_positives'] = int(tp)

    return metrics


def optimize_hyperparameters(X_train, y_train, n_trials=30):
    """
    Optimise les hyperparam√®tres de Random Forest avec Optuna.

    Args:
        X_train: Features d'entra√Ænement
        y_train: Cible d'entra√Ænement
        n_trials: Nombre d'essais Optuna (default: 30)

    Returns:
        Dictionnaire des meilleurs param√®tres
    """
    print(f"\nüîç Optimisation Optuna ({n_trials} trials)...")

    def objective(trial):
        """Fonction objectif pour Optuna."""
        # Sugg√©rer des hyperparam√®tres
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 50, 200),
            'max_depth': trial.suggest_int('max_depth', 5, 15),
            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),
            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),
            'class_weight': 'balanced',  # G√©rer le d√©s√©quilibre
            'random_state': 42
        }

        # Cr√©er et entra√Æner le mod√®le
        model = RandomForestClassifier(**params)
        model.fit(X_train, y_train)

        # √âvaluer sur le train set (avec validation crois√©e implicite)
        y_pred = model.predict(X_train)
        f1 = f1_score(y_train, y_pred)

        return f1

    # Cr√©er l'√©tude Optuna
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

    print(f"‚úì Optimisation termin√©e")
    print(f"  Meilleur F1-Score : {study.best_value:.4f}")
    print(f"  Meilleurs param√®tres :")
    for key, value in study.best_params.items():
        print(f"    {key}: {value}")

    return study.best_params


def train_model(X_train, y_train, params):
    """
    Entra√Æne le mod√®le Random Forest avec les param√®tres donn√©s.

    Args:
        X_train: Features d'entra√Ænement
        y_train: Cible d'entra√Ænement
        params: Hyperparam√®tres du mod√®le

    Returns:
        Mod√®le entra√Æn√©
    """
    # Ajouter class_weight si pas pr√©sent
    if 'class_weight' not in params:
        params['class_weight'] = 'balanced'

    # Ajouter random_state si pas pr√©sent
    if 'random_state' not in params:
        params['random_state'] = 42

    # Cr√©er et entra√Æner le mod√®le
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)

    print(f"‚úì Mod√®le Random Forest entra√Æn√©")

    return model


def evaluate_model(model, X_test, y_test):
    """
    √âvalue le mod√®le sur le test set.

    Args:
        model: Mod√®le entra√Æn√©
        X_test: Features de test
        y_test: Cible de test

    Returns:
        Dictionnaire avec les m√©triques
    """
    # Pr√©dictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Calculer les m√©triques
    metrics = calculate_metrics(y_test, y_pred, y_pred_proba)

    print(f"\nüìä R√©sultats sur le Test Set :")
    print(f"  Accuracy  : {metrics['accuracy']:.4f}")
    print(f"  Precision : {metrics['precision']:.4f}")
    print(f"  Recall    : {metrics['recall']:.4f} ‚≠ê")
    print(f"  F1-Score  : {metrics['f1_score']:.4f}")
    print(f"  ROC-AUC   : {metrics['roc_auc']:.4f}")
    print(f"\n  Confusion Matrix :")
    print(f"    TN={metrics['true_negatives']} | FP={metrics['false_positives']}")
    print(f"    FN={metrics['false_negatives']} | TP={metrics['true_positives']}")

    return metrics


def save_model(model, output_path="artifacts/models/random_forest_model.pkl"):
    """
    Sauvegarde le mod√®le sur disque.

    Args:
        model: Mod√®le √† sauvegarder
        output_path: Chemin de sauvegarde

    Returns:
        Chemin complet du fichier sauvegard√©
    """
    # Cr√©er le dossier si n√©cessaire
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)

    # Sauvegarder le mod√®le
    with open(output_path, 'wb') as f:
        pickle.dump(model, f)

    print(f"‚úì Mod√®le sauvegard√© : {output_path}")

    return output_path


def train_with_mlflow(X, y, experiment_name="Fraud Detection", n_trials=30):
    """
    Pipeline complet d'entra√Ænement avec tracking MLflow.

    Args:
        X: Features compl√®tes
        y: Cible compl√®te
        experiment_name: Nom de l'exp√©rience MLflow
        n_trials: Nombre d'essais Optuna

    Returns:
        Tuple (model, metrics)
    """
    print("\n" + "="*80)
    print("ENTRA√éNEMENT DU MOD√àLE RANDOM FOREST")
    print("="*80)

    # Configurer MLflow
    mlflow.set_experiment(experiment_name)

    # D√©marrer un run MLflow
    with mlflow.start_run():

        # 1. Split des donn√©es
        X_train, X_test, y_train, y_test = split_data(X, y)

        # Logger les infos du dataset
        mlflow.log_param("n_samples", len(X))
        mlflow.log_param("n_features", X.shape[1])
        mlflow.log_param("train_size", len(X_train))
        mlflow.log_param("test_size", len(X_test))
        mlflow.log_param("fraud_rate", f"{y.sum()/len(y)*100:.2f}%")

        # 2. Optimisation des hyperparam√®tres
        best_params = optimize_hyperparameters(X_train, y_train, n_trials=n_trials)

        # Logger les hyperparam√®tres
        for key, value in best_params.items():
            mlflow.log_param(key, value)

        # 3. Entra√Ænement du mod√®le
        model = train_model(X_train, y_train, best_params)

        # 4. √âvaluation
        metrics = evaluate_model(model, X_test, y_test)

        # Logger les m√©triques dans MLflow
        for key, value in metrics.items():
            mlflow.log_metric(key, value)

        # 5. Sauvegarder le mod√®le
        model_path = save_model(model)

        # Logger le mod√®le dans MLflow
        mlflow.sklearn.log_model(model, "model")
        mlflow.log_artifact(model_path)

        print("\n‚úì Entra√Ænement termin√© et track√© dans MLflow")
        print("="*80 + "\n")

        return model, metrics


if __name__ == "__main__":
    # Test du module
    from src.data.load_data import load_raw_data
    from src.preprocessing.preprocess import preprocess_data
    from src.features.build_features import build_features

    # Charger et pr√©parer les donn√©es
    df = load_raw_data()
    df_clean = preprocess_data(df)
    X, y = build_features(df_clean)

    # Entra√Æner avec MLflow
    model, metrics = train_with_mlflow(X, y, n_trials=10)  # 10 trials pour le test

    print(f"\n‚úì Test termin√© - F1-Score : {metrics['f1_score']:.4f}")

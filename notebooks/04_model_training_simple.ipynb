{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Entra√Ænement des Mod√®les (Version Simplifi√©e)\n",
    "\n",
    "Ce notebook teste 2 mod√®les :\n",
    "1. **Random Forest**\n",
    "2. **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:41:21.762572Z",
     "start_time": "2025-12-17T14:41:21.388253Z"
    }
   },
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "print(\"‚úì Imports OK\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremie.letarnec/work/lbc-auto-evaluation/mobility-market-values/pipelines/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:41:24.724656Z",
     "start_time": "2025-12-17T14:41:24.714230Z"
    }
   },
   "source": [
    "# Charger donn√©es\n",
    "X = pd.read_csv('../data/processed/X_features.csv')\n",
    "y = pd.read_csv('../data/processed/y_target.csv').values.ravel()\n",
    "print(f\"X: {X.shape}, y: {y.shape}\")\n",
    "print(f\"Fraudes: {(y==1).sum()/len(y)*100:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (10000, 10), y: (10000,)\n",
      "Fraudes: 5.51%\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:41:28.344978Z",
     "start_time": "2025-12-17T14:41:28.337346Z"
    }
   },
   "source": [
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scale_pos_weight = (y_train==0).sum() / (y_train==1).sum()\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8000, Test: 2000\n",
      "Scale pos weight: 17.14\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:41:36.650062Z",
     "start_time": "2025-12-17T14:41:36.641126Z"
    }
   },
   "source": [
    "# Fonction √©valuation\n",
    "def evaluate(model, X_test, y_test, name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    print(f\"\\n{'='*80}\\n{name}\\n{'='*80}\")\n",
    "    print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_test, y_pred):.4f} ‚≠ê\")\n",
    "    print(f\"F1:        {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "    print(f\"\\n{classification_report(y_test, y_pred, target_names=['Normal', 'Fraude'])}\")\n",
    "    \n",
    "    # Graphiques\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], xticklabels=['Normal', 'Fraude'], yticklabels=['Normal', 'Fraude'])\n",
    "    axes[0].set_title(f'Matrice - {name}', fontweight='bold')\n",
    "    axes[0].set_ylabel('Vraie classe')\n",
    "    axes[0].set_xlabel('Pr√©dite')\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    axes[1].plot(fpr, tpr, linewidth=2, label=f'AUC={roc_auc_score(y_test, y_proba):.4f}')\n",
    "    axes[1].plot([0,1], [0,1], 'k--', label='Random')\n",
    "    axes[1].set_title(f'ROC - {name}', fontweight='bold')\n",
    "    axes[1].set_xlabel('FPR')\n",
    "    axes[1].set_ylabel('TPR')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {'name': name, 'accuracy': accuracy_score(y_test, y_pred), 'precision': precision_score(y_test, y_pred), 'recall': recall_score(y_test, y_pred), 'f1': f1_score(y_test, y_pred), 'roc_auc': roc_auc_score(y_test, y_proba)}\n",
    "\n",
    "print(\"‚úì Fonction cr√©√©e\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fonction cr√©√©e\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-17T14:41:40.613733Z"
    }
   },
   "source": [
    "# RANDOM FOREST avec Optuna\n",
    "print(\"üå≤ RANDOM FOREST - Optuna (30 trials)\")\n",
    "def obj_rf(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'class_weight': 'balanced',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = RandomForestClassifier(**params)\n",
    "    return cross_val_score(model, X_train, y_train, cv=3, scoring='f1', n_jobs=-1).mean()\n",
    "\n",
    "study_rf = optuna.create_study(direction='maximize')\n",
    "study_rf.optimize(obj_rf, n_trials=30, show_progress_bar=True)\n",
    "print(f\"Meilleur F1 (CV): {study_rf.best_value:.4f}\")\n",
    "print(f\"Params: {study_rf.best_params}\")\n",
    "\n",
    "best_rf = RandomForestClassifier(**study_rf.best_params, class_weight='balanced', random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "results_rf = evaluate(best_rf, X_test, y_test, \"Random Forest\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-17 15:41:40,616] A new study created in memory with name: no-name-2889a524-aad6-4996-9c14-d44518a35962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≤ RANDOM FOREST - Optuna (30 trials)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST avec Optuna\n",
    "print(\"üöÄ XGBOOST - Optuna (30 trials)\")\n",
    "def obj_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "    model = XGBClassifier(**params)\n",
    "    return cross_val_score(model, X_train, y_train, cv=3, scoring='f1', n_jobs=-1).mean()\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(obj_xgb, n_trials=30, show_progress_bar=True)\n",
    "print(f\"Meilleur F1 (CV): {study_xgb.best_value:.4f}\")\n",
    "print(f\"Params: {study_xgb.best_params}\")\n",
    "\n",
    "best_xgb = XGBClassifier(**study_xgb.best_params, scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss')\n",
    "best_xgb.fit(X_train, y_train)\n",
    "results_xgb = evaluate(best_xgb, X_test, y_test, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARAISON\n",
    "comp = pd.DataFrame([results_rf, results_xgb]).set_index('name')\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAISON\")\n",
    "print(\"=\"*80)\n",
    "print(comp.round(4))\n",
    "best_name = comp['recall'].idxmax()\n",
    "print(f\"\\nüèÜ MEILLEUR (Recall): {best_name}\")\n",
    "\n",
    "comp.plot(kind='bar', figsize=(12, 5))\n",
    "plt.title('Comparaison', fontweight='bold')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des features\n",
    "best_model = best_rf if best_name == 'Random Forest' else best_xgb\n",
    "feat_imp = pd.DataFrame({'feature': X.columns, 'importance': best_model.feature_importances_}).sort_values('importance', ascending=False)\n",
    "print(f\"\\nüìä Importance - {best_name}\")\n",
    "print(feat_imp.to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feat_imp['feature'], feat_imp['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title(f'Features - {best_name}', fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "import pickle\n",
    "model_path = f'../artifacts/models/best_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"‚úì Mod√®le sauvegard√©: {model_path}\")\n",
    "comp.to_csv('../artifacts/metrics/comparison.csv')\n",
    "print(\"‚úì M√©triques sauvegard√©es\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
